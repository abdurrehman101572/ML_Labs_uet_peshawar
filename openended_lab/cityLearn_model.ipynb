{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29cef70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'G:\\Abdur_rehman_ML') \n",
    "from stlf_torch_kit import  DataLoadeing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, time\n",
    "from stlf_torch_kit import univariate_multi_step\n",
    "from stlf_torch_kit import SaveBestModel, PlotLossCurves, LoadModel, train, TestModel, BatchGenerator, results\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc31f46",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2a9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6132, 29), (1752, 29), (876, 29))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_dataset ='G:\\Abdur_rehman_ML\\Dataset'\n",
    "path_tr = os.path.join(path_dataset, 'CityLearn_train.csv')\n",
    "df_tr = pd.read_csv(path_tr)\n",
    "train_set = df_tr.iloc[:].values\n",
    "path_v = os.path.join(path_dataset, 'CityLearn_validation.csv')\n",
    "df_v = pd.read_csv(path_v)\n",
    "validation_set = df_v.iloc[:].values \n",
    "path_te = os.path.join(path_dataset, 'Citylearn_test.csv')\n",
    "df_te = pd.read_csv(path_te)\n",
    "test_set = df_te.iloc[:].values \n",
    "\n",
    "path_scaler = os.path.join(path_dataset, 'Scaler.pkl')\n",
    "scaler         = pickle.load(open(path_scaler, 'rb'))\n",
    "\n",
    "train_set.shape, validation_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293ad9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumed 0.07294917106628418 sec\n"
     ]
    }
   ],
   "source": [
    "time_steps = 24 \n",
    "target_len = 1 #how much steps do you wana forecast #Edit\n",
    "start = time.time()\n",
    "train_X , train_y = univariate_multi_step(train_set, time_steps, target_col=0,target_len=target_len)\n",
    "validation_X, validation_y = univariate_multi_step(validation_set, time_steps, target_col=0,target_len=target_len)\n",
    "test_X, test_y = univariate_multi_step(test_set, time_steps, target_col=0,target_len=target_len)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc241223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851, 24, 29)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52558d",
   "metadata": {},
   "source": [
    "#### LSTM_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1912c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, time_steps=24, num_features=29):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=num_features, hidden_size=64, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=32, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, time_steps, num_features)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Only take the output of the last time step\n",
    "        x = x[:, -1, :]  # shape: (batch_size, 32)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659def4",
   "metadata": {},
   "source": [
    "# instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1f740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel()#Edit\n",
    "criterion = nn.MSELoss() #Edit, don't change\n",
    "\n",
    "save_best_model = SaveBestModel()\n",
    "Plot_Loss=PlotLossCurves()\n",
    "load_model=LoadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a217f",
   "metadata": {},
   "source": [
    "# Learning Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9867523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=0.001 # Edit\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr) #Edit\n",
    "lr = 0.001  # Initial learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)  # Added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b55ecf",
   "metadata": {},
   "source": [
    "# Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53206b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device\n",
    "device = get_model_device(model)\n",
    "print(\"Model is on device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fafebb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa963e",
   "metadata": {},
   "source": [
    "#### Path & other Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3924df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "num_epochs = 100 #Edit\n",
    "best_model_path=r'G:\\Abdur_rehman_ML\\chk'+str('\\\\') #Edit\n",
    "fig_path=r'G:\\Abdur_rehman_ML\\chk' #Edit\n",
    "train_data_loader, validation_data_loader, test_data_loader = DataLoadeing(train_X ,\n",
    "                                                                           train_y, \n",
    "                                                                           validation_X, \n",
    "                                                                           validation_y, \n",
    "                                                                           test_X, \n",
    "                                                                           test_y, \n",
    "                                                                           batch_size=32) #Batch_Size Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f460874b",
   "metadata": {},
   "source": [
    "#### Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b17e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Edit, for Now Don't  Change\n",
    "\n",
    "criterion_mae = nn.L1Loss()\n",
    "\n",
    "save_best_model = SaveBestModel()\n",
    "Plot_Loss=PlotLossCurves()\n",
    "load_model=LoadModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bee91",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37aa742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [191/191], Training Loss: 0.0624\n",
      "Epoch [1/100], Step [54/54], Val Loss: 0.0405\n",
      "\n",
      "Saving best model for epoch: 1\n",
      " at G:\\Abdur_rehman_ML\\chk\\1best_model.pth\n",
      "Epoch [2/100], Step [191/191], Training Loss: 0.0423\n",
      "Epoch [2/100], Step [54/54], Val Loss: 0.0408\n",
      "Epoch [3/100], Step [191/191], Training Loss: 0.0410\n",
      "Epoch [3/100], Step [54/54], Val Loss: 0.0400\n",
      "\n",
      "Saving best model for epoch: 3\n",
      " at G:\\Abdur_rehman_ML\\chk\\3best_model.pth\n",
      "Epoch [4/100], Step [191/191], Training Loss: 0.0391\n",
      "Epoch [4/100], Step [54/54], Val Loss: 0.0402\n",
      "Epoch [5/100], Step [191/191], Training Loss: 0.0380\n",
      "Epoch [5/100], Step [54/54], Val Loss: 0.0401\n",
      "Epoch [6/100], Step [191/191], Training Loss: 0.0371\n",
      "Epoch [6/100], Step [54/54], Val Loss: 0.0403\n",
      "Epoch [7/100], Step [191/191], Training Loss: 0.0356\n",
      "Epoch [7/100], Step [54/54], Val Loss: 0.0399\n",
      "\n",
      "Saving best model for epoch: 7\n",
      " at G:\\Abdur_rehman_ML\\chk\\7best_model.pth\n",
      "Epoch [8/100], Step [191/191], Training Loss: 0.0344\n",
      "Epoch [8/100], Step [54/54], Val Loss: 0.0397\n",
      "\n",
      "Saving best model for epoch: 8\n",
      " at G:\\Abdur_rehman_ML\\chk\\8best_model.pth\n",
      "Epoch [9/100], Step [191/191], Training Loss: 0.0328\n",
      "Epoch [9/100], Step [54/54], Val Loss: 0.0402\n",
      "Epoch [10/100], Step [191/191], Training Loss: 0.0312\n",
      "Epoch [10/100], Step [54/54], Val Loss: 0.0385\n",
      "\n",
      "Saving best model for epoch: 10\n",
      " at G:\\Abdur_rehman_ML\\chk\\10best_model.pth\n",
      "Epoch [11/100], Step [191/191], Training Loss: 0.0290\n",
      "Epoch [11/100], Step [54/54], Val Loss: 0.0370\n",
      "\n",
      "Saving best model for epoch: 11\n",
      " at G:\\Abdur_rehman_ML\\chk\\11best_model.pth\n",
      "Epoch [12/100], Step [191/191], Training Loss: 0.0275\n",
      "Epoch [12/100], Step [54/54], Val Loss: 0.0360\n",
      "\n",
      "Saving best model for epoch: 12\n",
      " at G:\\Abdur_rehman_ML\\chk\\12best_model.pth\n",
      "Epoch [13/100], Step [191/191], Training Loss: 0.0258\n",
      "Epoch [13/100], Step [54/54], Val Loss: 0.0339\n",
      "\n",
      "Saving best model for epoch: 13\n",
      " at G:\\Abdur_rehman_ML\\chk\\13best_model.pth\n",
      "Epoch [14/100], Step [191/191], Training Loss: 0.0255\n",
      "Epoch [14/100], Step [54/54], Val Loss: 0.0323\n",
      "\n",
      "Saving best model for epoch: 14\n",
      " at G:\\Abdur_rehman_ML\\chk\\14best_model.pth\n",
      "Epoch [15/100], Step [191/191], Training Loss: 0.0242\n",
      "Epoch [15/100], Step [54/54], Val Loss: 0.0311\n",
      "\n",
      "Saving best model for epoch: 15\n",
      " at G:\\Abdur_rehman_ML\\chk\\15best_model.pth\n",
      "Epoch [16/100], Step [191/191], Training Loss: 0.0233\n",
      "Epoch [16/100], Step [54/54], Val Loss: 0.0296\n",
      "\n",
      "Saving best model for epoch: 16\n",
      " at G:\\Abdur_rehman_ML\\chk\\16best_model.pth\n",
      "Epoch [17/100], Step [191/191], Training Loss: 0.0227\n",
      "Epoch [17/100], Step [54/54], Val Loss: 0.0296\n",
      "\n",
      "Saving best model for epoch: 17\n",
      " at G:\\Abdur_rehman_ML\\chk\\17best_model.pth\n",
      "Epoch [18/100], Step [191/191], Training Loss: 0.0223\n",
      "Epoch [18/100], Step [54/54], Val Loss: 0.0289\n",
      "\n",
      "Saving best model for epoch: 18\n",
      " at G:\\Abdur_rehman_ML\\chk\\18best_model.pth\n",
      "Epoch [19/100], Step [191/191], Training Loss: 0.0222\n",
      "Epoch [19/100], Step [54/54], Val Loss: 0.0282\n",
      "\n",
      "Saving best model for epoch: 19\n",
      " at G:\\Abdur_rehman_ML\\chk\\19best_model.pth\n",
      "Epoch [20/100], Step [191/191], Training Loss: 0.0221\n",
      "Epoch [20/100], Step [54/54], Val Loss: 0.0283\n",
      "Epoch [21/100], Step [191/191], Training Loss: 0.0214\n",
      "Epoch [21/100], Step [54/54], Val Loss: 0.0281\n",
      "\n",
      "Saving best model for epoch: 21\n",
      " at G:\\Abdur_rehman_ML\\chk\\21best_model.pth\n",
      "Epoch [22/100], Step [191/191], Training Loss: 0.0212\n",
      "Epoch [22/100], Step [54/54], Val Loss: 0.0277\n",
      "\n",
      "Saving best model for epoch: 22\n",
      " at G:\\Abdur_rehman_ML\\chk\\22best_model.pth\n",
      "Epoch [23/100], Step [191/191], Training Loss: 0.0207\n",
      "Epoch [23/100], Step [54/54], Val Loss: 0.0278\n",
      "Epoch [24/100], Step [191/191], Training Loss: 0.0207\n",
      "Epoch [24/100], Step [54/54], Val Loss: 0.0280\n",
      "Epoch [25/100], Step [191/191], Training Loss: 0.0201\n",
      "Epoch [25/100], Step [54/54], Val Loss: 0.0291\n",
      "Epoch [26/100], Step [191/191], Training Loss: 0.0202\n",
      "Epoch [26/100], Step [54/54], Val Loss: 0.0286\n",
      "Epoch [27/100], Step [191/191], Training Loss: 0.0196\n",
      "Epoch [27/100], Step [54/54], Val Loss: 0.0284\n",
      "Epoch [28/100], Step [191/191], Training Loss: 0.0198\n",
      "Epoch [28/100], Step [54/54], Val Loss: 0.0291\n",
      "Epoch [29/100], Step [191/191], Training Loss: 0.0193\n",
      "Epoch [29/100], Step [54/54], Val Loss: 0.0293\n",
      "Epoch [30/100], Step [191/191], Training Loss: 0.0193\n",
      "Epoch [30/100], Step [54/54], Val Loss: 0.0306\n",
      "Epoch [31/100], Step [191/191], Training Loss: 0.0190\n",
      "Epoch [31/100], Step [54/54], Val Loss: 0.0307\n",
      "Epoch [32/100], Step [191/191], Training Loss: 0.0184\n",
      "Epoch [32/100], Step [54/54], Val Loss: 0.0298\n",
      "Epoch [33/100], Step [191/191], Training Loss: 0.0180\n",
      "Epoch [33/100], Step [54/54], Val Loss: 0.0305\n",
      "Epoch [34/100], Step [191/191], Training Loss: 0.0180\n",
      "Epoch [34/100], Step [54/54], Val Loss: 0.0311\n",
      "Epoch [35/100], Step [191/191], Training Loss: 0.0176\n",
      "Epoch [35/100], Step [54/54], Val Loss: 0.0299\n",
      "Epoch [36/100], Step [191/191], Training Loss: 0.0181\n",
      "Epoch [36/100], Step [54/54], Val Loss: 0.0327\n",
      "Epoch [37/100], Step [191/191], Training Loss: 0.0178\n",
      "Epoch [37/100], Step [54/54], Val Loss: 0.0333\n",
      "Epoch [38/100], Step [191/191], Training Loss: 0.0173\n",
      "Epoch [38/100], Step [54/54], Val Loss: 0.0336\n",
      "Epoch [39/100], Step [191/191], Training Loss: 0.0173\n",
      "Epoch [39/100], Step [54/54], Val Loss: 0.0325\n",
      "Epoch [40/100], Step [191/191], Training Loss: 0.0165\n",
      "Epoch [40/100], Step [54/54], Val Loss: 0.0329\n",
      "Epoch [41/100], Step [191/191], Training Loss: 0.0165\n",
      "Epoch [41/100], Step [54/54], Val Loss: 0.0355\n",
      "Epoch [42/100], Step [191/191], Training Loss: 0.0163\n",
      "Epoch [42/100], Step [54/54], Val Loss: 0.0359\n",
      "Epoch [43/100], Step [191/191], Training Loss: 0.0161\n",
      "Epoch [43/100], Step [54/54], Val Loss: 0.0350\n",
      "Epoch [44/100], Step [191/191], Training Loss: 0.0159\n",
      "Epoch [44/100], Step [54/54], Val Loss: 0.0337\n",
      "Epoch [45/100], Step [191/191], Training Loss: 0.0149\n",
      "Epoch [45/100], Step [54/54], Val Loss: 0.0348\n",
      "Epoch [46/100], Step [191/191], Training Loss: 0.0149\n",
      "Epoch [46/100], Step [54/54], Val Loss: 0.0345\n",
      "Epoch [47/100], Step [191/191], Training Loss: 0.0152\n",
      "Epoch [47/100], Step [54/54], Val Loss: 0.0356\n",
      "Epoch [48/100], Step [191/191], Training Loss: 0.0143\n",
      "Epoch [48/100], Step [54/54], Val Loss: 0.0337\n",
      "Epoch [49/100], Step [191/191], Training Loss: 0.0140\n",
      "Epoch [49/100], Step [54/54], Val Loss: 0.0353\n",
      "Epoch [50/100], Step [191/191], Training Loss: 0.0140\n",
      "Epoch [50/100], Step [54/54], Val Loss: 0.0362\n",
      "Epoch [51/100], Step [191/191], Training Loss: 0.0137\n",
      "Epoch [51/100], Step [54/54], Val Loss: 0.0354\n",
      "Epoch [52/100], Step [191/191], Training Loss: 0.0136\n",
      "Epoch [52/100], Step [54/54], Val Loss: 0.0359\n",
      "Epoch [53/100], Step [191/191], Training Loss: 0.0135\n",
      "Epoch [53/100], Step [54/54], Val Loss: 0.0361\n",
      "Epoch [54/100], Step [191/191], Training Loss: 0.0136\n",
      "Epoch [54/100], Step [54/54], Val Loss: 0.0365\n",
      "Epoch [55/100], Step [191/191], Training Loss: 0.0128\n",
      "Epoch [55/100], Step [54/54], Val Loss: 0.0353\n",
      "Epoch [56/100], Step [191/191], Training Loss: 0.0126\n",
      "Epoch [56/100], Step [54/54], Val Loss: 0.0369\n",
      "Epoch [57/100], Step [191/191], Training Loss: 0.0127\n",
      "Epoch [57/100], Step [54/54], Val Loss: 0.0364\n",
      "Epoch [58/100], Step [191/191], Training Loss: 0.0126\n",
      "Epoch [58/100], Step [54/54], Val Loss: 0.0366\n",
      "Epoch [59/100], Step [191/191], Training Loss: 0.0128\n",
      "Epoch [59/100], Step [54/54], Val Loss: 0.0363\n",
      "Epoch [60/100], Step [191/191], Training Loss: 0.0127\n",
      "Epoch [60/100], Step [54/54], Val Loss: 0.0353\n",
      "Epoch [61/100], Step [191/191], Training Loss: 0.0131\n",
      "Epoch [61/100], Step [54/54], Val Loss: 0.0345\n",
      "Epoch [62/100], Step [191/191], Training Loss: 0.0118\n",
      "Epoch [62/100], Step [54/54], Val Loss: 0.0368\n",
      "Epoch [63/100], Step [191/191], Training Loss: 0.0121\n",
      "Epoch [63/100], Step [54/54], Val Loss: 0.0368\n",
      "Epoch [64/100], Step [191/191], Training Loss: 0.0121\n",
      "Epoch [64/100], Step [54/54], Val Loss: 0.0370\n",
      "Epoch [65/100], Step [191/191], Training Loss: 0.0120\n",
      "Epoch [65/100], Step [54/54], Val Loss: 0.0369\n",
      "Epoch [66/100], Step [191/191], Training Loss: 0.0115\n",
      "Epoch [66/100], Step [54/54], Val Loss: 0.0381\n",
      "Epoch [67/100], Step [191/191], Training Loss: 0.0112\n",
      "Epoch [67/100], Step [54/54], Val Loss: 0.0371\n",
      "Epoch [68/100], Step [191/191], Training Loss: 0.0112\n",
      "Epoch [68/100], Step [54/54], Val Loss: 0.0368\n",
      "Epoch [69/100], Step [191/191], Training Loss: 0.0111\n",
      "Epoch [69/100], Step [54/54], Val Loss: 0.0376\n",
      "Epoch [70/100], Step [191/191], Training Loss: 0.0106\n",
      "Epoch [70/100], Step [54/54], Val Loss: 0.0378\n",
      "Epoch [71/100], Step [191/191], Training Loss: 0.0111\n",
      "Epoch [71/100], Step [54/54], Val Loss: 0.0372\n",
      "Epoch [72/100], Step [191/191], Training Loss: 0.0117\n",
      "Epoch [72/100], Step [54/54], Val Loss: 0.0370\n",
      "Epoch [73/100], Step [191/191], Training Loss: 0.0110\n",
      "Epoch [73/100], Step [54/54], Val Loss: 0.0368\n",
      "Epoch [74/100], Step [191/191], Training Loss: 0.0105\n",
      "Epoch [74/100], Step [54/54], Val Loss: 0.0370\n",
      "Epoch [75/100], Step [191/191], Training Loss: 0.0107\n",
      "Epoch [75/100], Step [54/54], Val Loss: 0.0370\n",
      "Epoch [76/100], Step [191/191], Training Loss: 0.0107\n",
      "Epoch [76/100], Step [54/54], Val Loss: 0.0376\n",
      "Epoch [77/100], Step [191/191], Training Loss: 0.0104\n",
      "Epoch [77/100], Step [54/54], Val Loss: 0.0383\n",
      "Epoch [78/100], Step [191/191], Training Loss: 0.0100\n",
      "Epoch [78/100], Step [54/54], Val Loss: 0.0381\n",
      "Epoch [79/100], Step [191/191], Training Loss: 0.0103\n",
      "Epoch [79/100], Step [54/54], Val Loss: 0.0376\n",
      "Epoch [80/100], Step [191/191], Training Loss: 0.0104\n",
      "Epoch [80/100], Step [54/54], Val Loss: 0.0378\n",
      "Epoch [81/100], Step [191/191], Training Loss: 0.0099\n",
      "Epoch [81/100], Step [54/54], Val Loss: 0.0369\n",
      "Epoch [82/100], Step [191/191], Training Loss: 0.0102\n",
      "Epoch [82/100], Step [54/54], Val Loss: 0.0381\n",
      "Epoch [83/100], Step [191/191], Training Loss: 0.0096\n",
      "Epoch [83/100], Step [54/54], Val Loss: 0.0378\n",
      "Epoch [84/100], Step [191/191], Training Loss: 0.0096\n",
      "Epoch [84/100], Step [54/54], Val Loss: 0.0398\n",
      "Epoch [85/100], Step [191/191], Training Loss: 0.0096\n",
      "Epoch [85/100], Step [54/54], Val Loss: 0.0380\n",
      "Epoch [86/100], Step [191/191], Training Loss: 0.0095\n",
      "Epoch [86/100], Step [54/54], Val Loss: 0.0383\n",
      "Epoch [87/100], Step [191/191], Training Loss: 0.0095\n",
      "Epoch [87/100], Step [54/54], Val Loss: 0.0377\n",
      "Epoch [88/100], Step [191/191], Training Loss: 0.0098\n",
      "Epoch [88/100], Step [54/54], Val Loss: 0.0386\n",
      "Epoch [89/100], Step [191/191], Training Loss: 0.0095\n",
      "Epoch [89/100], Step [54/54], Val Loss: 0.0386\n",
      "Epoch [90/100], Step [191/191], Training Loss: 0.0094\n",
      "Epoch [90/100], Step [54/54], Val Loss: 0.0364\n",
      "Epoch [91/100], Step [191/191], Training Loss: 0.0100\n",
      "Epoch [91/100], Step [54/54], Val Loss: 0.0380\n",
      "Epoch [92/100], Step [191/191], Training Loss: 0.0091\n",
      "Epoch [92/100], Step [54/54], Val Loss: 0.0378\n",
      "Epoch [93/100], Step [191/191], Training Loss: 0.0084\n",
      "Epoch [93/100], Step [54/54], Val Loss: 0.0379\n",
      "Epoch [94/100], Step [191/191], Training Loss: 0.0085\n",
      "Epoch [94/100], Step [54/54], Val Loss: 0.0400\n",
      "Epoch [95/100], Step [191/191], Training Loss: 0.0086\n",
      "Epoch [95/100], Step [54/54], Val Loss: 0.0381\n",
      "Epoch [96/100], Step [191/191], Training Loss: 0.0090\n",
      "Epoch [96/100], Step [54/54], Val Loss: 0.0393\n",
      "Epoch [97/100], Step [191/191], Training Loss: 0.0085\n",
      "Epoch [97/100], Step [54/54], Val Loss: 0.0399\n",
      "Epoch [98/100], Step [191/191], Training Loss: 0.0084\n",
      "Epoch [98/100], Step [54/54], Val Loss: 0.0390\n",
      "Epoch [99/100], Step [191/191], Training Loss: 0.0086\n",
      "Epoch [99/100], Step [54/54], Val Loss: 0.0393\n",
      "Epoch [100/100], Step [191/191], Training Loss: 0.0086\n",
      "Epoch [100/100], Step [54/54], Val Loss: 0.0372\n",
      "Time Consumed 945.9961321353912 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(start_epoch,\n",
    "      num_epochs,\n",
    "      best_model_path,\n",
    "      fig_path,\n",
    "      model,criterion,optimizer,save_best_model,Plot_Loss,\n",
    "      train_data_loader,\n",
    "      validation_data_loader)\n",
    "print('Time Consumed', time.time()-start, \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c897f0",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e071d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lr = 0.001\n",
      "Time Consumed 0.38401293754577637 sec\n",
      "Mean Absolute Error (MAE): 0.38\n",
      "Median Absolute Error (MedAE): 0.29\n",
      "Mean Squared Error (MSE): 0.26\n",
      "Root Mean Squared Error (RMSE): 0.51\n",
      "Mean Absolute Percentage Error (MAPE): 57.74 %\n",
      "Median Absolute Percentage Error (MDAPE): 40.52 %\n",
      "\n",
      "\n",
      "y_test_unscaled.shape=  (851, 1)\n",
      "y_pred.shape=  (851, 1)\n"
     ]
    }
   ],
   "source": [
    "load_model_path=r'G:\\Abdur_rehman_ML\\chk\\22best_model.pth' # Edit\n",
    "test_model=TestModel()\n",
    "start = time.time()\n",
    "y_pred_scaled=test_model(model, test_X,load_model,load_model_path,lr)\n",
    "print('Time Consumed', time.time()-start, \"sec\")\n",
    "results(scaler, y_pred_scaled,test_y)\n",
    "\n",
    "# MAPE, MAE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ff80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STLF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
